{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMWF4Ojz4jaM0Pyph4s9NEi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassaku12/manabiDX2025/blob/main/%E6%BC%94%E7%BF%9203%EF%BC%88%E8%A9%A6%E3%81%99%E7%94%A8%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å–ã‚Š"
      ],
      "metadata": {
        "id": "5RWmI06ynebb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V2u0gVJvouG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188ca0c1-4c44-484f-ac34-c016c25f6cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ã¾ãšã¯Googleãƒ‰ãƒ©ã‚¤ãƒ–ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# japanize-matplotlibã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (å®Ÿè¡Œç’°å¢ƒã«æœªå°å…¥ã®å ´åˆ)\n",
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCJXCkY4EUNg",
        "outputId": "bb9dada3-6b86-4848-a17f-30287cb55121"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/4.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from japanize-matplotlib) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=7707fa96c61a2f263da1e435419d89b95954246990d71e7b1ed4d4a7a59cec8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/f7/9b/418f19a7b9340fc16e071e89efc379aca68d40238b258df53d\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import japanize_matplotlib\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "o6mDl66NEpqI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/ãƒãƒŠãƒ’ã‚™DX'"
      ],
      "metadata": {
        "id": "PB_duKNHFzL9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "train_df = pd.read_csv(base_dir + '/train.csv')\n",
        "test_df = pd.read_csv(base_dir + '/test.csv')\n",
        "submission_df = pd.read_csv(base_dir + '/sample_submit.csv', header=None)\n",
        "assessment_df = pd.read_csv(base_dir + '/é©åˆæ€§åˆ¤å®šã‚·ãƒ¼ãƒˆä¸€è¦§è¡¨.csv')"
      ],
      "metadata": {
        "id": "T2oQ0Gv7GEQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93968ef2-43de-4077-cc2d-1a3c1a493ff3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š è¿½åŠ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "\n",
        "# ç´„å®šãƒ‡ãƒ¼ã‚¿ã¨å•†å“ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿\n",
        "try:\n",
        "    trading_df = pd.read_csv(base_dir + '/ç´„å®šãƒ‡ãƒ¼ã‚¿ä¸€è¦§è¡¨.csv')\n",
        "    product_df = pd.read_csv(base_dir + '/å•†å“ãƒªã‚¹ãƒˆ.csv')\n",
        "\n",
        "    print(f\"âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿: {trading_df.shape}\")\n",
        "    print(f\"âœ… å•†å“ãƒªã‚¹ãƒˆ: {product_df.shape}\")\n",
        "    print(f\"âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿åˆ—: {list(trading_df.columns)}\")\n",
        "    print(f\"âœ… å•†å“ãƒªã‚¹ãƒˆåˆ—: {list(product_df.columns)}\")\n",
        "\n",
        "    HAS_ALL_DATA = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    HAS_ALL_DATA = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E5vkA4W3Wu3",
        "outputId": "1a430186-4c84-4af6-e873-fb7359c7d7b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š è¿½åŠ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\n",
            "âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿: (37514, 20)\n",
            "âœ… å•†å“ãƒªã‚¹ãƒˆ: (786, 18)\n",
            "âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿åˆ—: ['å–å¼•æ—¥', 'é¡§å®¢ID', 'å–å¼•ã‚³ãƒ¼ãƒ‰', 'å•†å“å', 'å–å¾—ä¾¡é¡', 'å–å¾—å˜ä¾¡', 'å–å¼•æ•°é‡', 'å£²å´å˜ä¾¡', 'å£²å´ä¾¡é¡', 'å£²å´æç›Š', 'å„Ÿé‚„ä¾¡é¡', 'å„Ÿé‚„å˜ä¾¡', 'å„Ÿé‚„æç›Š', 'ç©ç«‹æŠ•è³‡è³¼å…¥ã®æ–°è¦/æ—¢å­˜', 'è³¼å…¥é–‹å§‹å¹´æœˆ', 'è³¼å…¥çµ‚äº†å¹´æœˆ', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å–å¼•ãƒ•ãƒ©ã‚°', 'ã‚´ãƒ¼ãƒ«è¨­å®šå®Ÿæ–½', 'ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šå®Ÿæ–½', 'ãƒ­ã‚¹ã‚«ãƒƒãƒˆæ°´æº–']\n",
            "âœ… å•†å“ãƒªã‚¹ãƒˆåˆ—: ['å•†å“å', 'å•†å“ã‚«ãƒ†ã‚´ãƒª', 'ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'é€šè²¨', 'å‹§èª˜ç•™æ„å•†å“', 'è§£ç´„æ‰‹æ•°æ–™ç‡', 'ç™ºè¡Œæ—¥', 'å„Ÿé‚„æ—¥', 'åˆ©ç‡', 'ç™ºè¡Œæ—¥æ ªä¾¡', 'æ—©æœŸå„Ÿé‚„æ ªä¾¡', 'ãƒãƒƒã‚¯ã‚¤ãƒ³æ ªä¾¡', 'æ—©æœŸå„Ÿé‚„æ—¥', 'ãƒãƒƒã‚¯ã‚¤ãƒ³æ—¥', 'å¥‘ç´„å¹´æœˆ', 'äºˆå®šåˆ©ç‡', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ°é”æ—¥']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã¨ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆã™ã¹ã¦ã‚’é–¢æ•°ã§çµ±ä¸€ï¼‰"
      ],
      "metadata": {
        "id": "AhZBUc-aJyJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é¡§å®¢å±æ€§ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
        "assessment_df['å–å¼•æ—¥'] = pd.to_datetime(assessment_df['å–å¼•æ—¥'])\n",
        "assessment_df_latest = assessment_df.sort_values(['é¡§å®¢ID', 'å–å¼•æ—¥']).drop_duplicates(subset='é¡§å®¢ID', keep='last')\n",
        "assessment_df_selected = assessment_df_latest[['é¡§å®¢ID', 'é¡§å®¢å¹´é½¢', 'æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰']]"
      ],
      "metadata": {
        "id": "geqgG3BxJ0I6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é¡§å®¢å±æ€§æƒ…å ±ã‚’çµåˆ\n",
        "train_df = pd.merge(train_df, assessment_df_selected, on='é¡§å®¢ID', how='left')\n",
        "test_df = pd.merge(test_df, assessment_df_selected, on='é¡§å®¢ID', how='left')"
      ],
      "metadata": {
        "id": "xs6QOxbEKTNB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ¬ æå€¤ã‚’è£œå®Œ\n",
        "train_df['é¡§å®¢å¹´é½¢'] = train_df['é¡§å®¢å¹´é½¢'].fillna(train_df['é¡§å®¢å¹´é½¢'].mean())\n",
        "test_df['é¡§å®¢å¹´é½¢'] = test_df['é¡§å®¢å¹´é½¢'].fillna(test_df['é¡§å®¢å¹´é½¢'].mean())\n",
        "train_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'] = train_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'].fillna(0)\n",
        "test_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'] = test_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'].fillna(0)"
      ],
      "metadata": {
        "id": "TheHXUiyKVQm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# åŸºæº–å¹´æœˆã‹ã‚‰å¹´ã¨æœˆã‚’æŠ½å‡º\n",
        "train_df['year'] = train_df['åŸºæº–å¹´æœˆ'].str.split('-').str[0].astype(int)\n",
        "train_df['month'] = train_df['åŸºæº–å¹´æœˆ'].str.split('-').str[1].astype(int)\n",
        "test_df['year'] = test_df['åŸºæº–å¹´æœˆ'].str.split('-').str[0].astype(int)\n",
        "test_df['month'] = test_df['åŸºæº–å¹´æœˆ'].str.split('-').str[1].astype(int)"
      ],
      "metadata": {
        "id": "cwCZR8nTKYEw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_complete_features(df):\n",
        "    \"\"\"å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰\"\"\"\n",
        "    df = df.copy()\n",
        "    print(\"ğŸš€ å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆé–‹å§‹\")\n",
        "\n",
        "    # 1. åŸºæœ¬ç‰¹å¾´é‡\n",
        "    df['è³‡ç”£è¦æ¨¡'] = df['å–å¾—ä¾¡é¡'] + df['æ™‚ä¾¡ä¾¡é¡']\n",
        "    df['æŠ•è³‡åŠ¹ç‡'] = df['æ™‚ä¾¡ä¾¡é¡'] / (df['å–å¾—ä¾¡é¡'] + 1)\n",
        "    df['å¹´é½¢èª¿æ•´è³‡ç”£'] = df['æ™‚ä¾¡ä¾¡é¡'] / (df['é¡§å®¢å¹´é½¢'] + 1)\n",
        "    df['é¡§å®¢ãƒ©ãƒ³ã‚¯'] = df['æ™‚ä¾¡ä¾¡é¡'].rank(pct=True)\n",
        "    df['å¹´é½¢Ã—è³‡ç”£è¦æ¨¡'] = df['é¡§å®¢å¹´é½¢'] * df['è³‡ç”£è¦æ¨¡']\n",
        "    df['å«ã¿æç›Šç‡'] = np.where(df['å–å¾—ä¾¡é¡'] != 0,\n",
        "                                (df['æ™‚ä¾¡ä¾¡é¡'] - df['å–å¾—ä¾¡é¡']) / df['å–å¾—ä¾¡é¡'], 0)\n",
        "\n",
        "    if HAS_ALL_DATA:\n",
        "        # 2. å®Ÿéš›ã®å–å¼•ç‰¹å¾´é‡\n",
        "        print(\"ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¼•ç‰¹å¾´é‡ä½œæˆ\")\n",
        "        df = add_real_trading_features(df)\n",
        "\n",
        "        # 3. å®Ÿéš›ã®å•†å“ç‰¹å¾´é‡\n",
        "        print(\"ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å•†å“ç‰¹å¾´é‡ä½œæˆ\")\n",
        "        df = add_real_product_features(df)\n",
        "    else:\n",
        "        print(\"âš ï¸ ãƒ€ãƒŸãƒ¼ç‰¹å¾´é‡ã§ç¶™ç¶š\")\n",
        "        df = add_dummy_features(df)\n",
        "\n",
        "    # 4. ãƒ•ãƒ©ã‚°ç‰¹å¾´é‡\n",
        "    df['é«˜è³‡ç”£ãƒ•ãƒ©ã‚°'] = (df['è³‡ç”£è¦æ¨¡'] > df['è³‡ç”£è¦æ¨¡'].quantile(0.8)).astype(int)\n",
        "    df['å«ã¿æãƒ•ãƒ©ã‚°'] = (df['è©•ä¾¡æç›Š'] < 0).astype(int)\n",
        "    df['ã‚·ãƒ‹ã‚¢ãƒ•ãƒ©ã‚°'] = (df['é¡§å®¢å¹´é½¢'] >= 65).astype(int)\n",
        "    df['æ ªå¼çµŒé¨“ã‚ã‚Š'] = (df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'] > 0).astype(int)\n",
        "\n",
        "    print(\"âœ… å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆå®Œäº†\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "lW_5t208Ka2H"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_real_trading_features(df):\n",
        "    \"\"\"å®Ÿéš›ã®ç´„å®šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
        "    try:\n",
        "        # å–å¼•æ—¥ã‚’æ—¥ä»˜å‹ã«å¤‰æ›\n",
        "        trading_df['å–å¼•æ—¥'] = pd.to_datetime(trading_df['å–å¼•æ—¥'])\n",
        "\n",
        "        # 2021å¹´11æœˆ30æ—¥ä»¥å‰ã®å–å¼•ã®ã¿ï¼ˆäºˆæ¸¬ã«ä½¿ãˆã‚‹éå»ãƒ‡ãƒ¼ã‚¿ï¼‰\n",
        "        past_trading = trading_df[trading_df['å–å¼•æ—¥'] <= '2021-11-30']\n",
        "\n",
        "        print(f\"   éå»å–å¼•ãƒ‡ãƒ¼ã‚¿: {past_trading.shape}\")\n",
        "\n",
        "        # é¡§å®¢åˆ¥é›†è¨ˆ\n",
        "        customer_stats = past_trading.groupby('é¡§å®¢ID').agg({\n",
        "            'å–å¾—ä¾¡é¡': ['count', 'sum', 'mean'],  # å–å¼•å›æ•°ã€ç·é¡ã€å¹³å‡\n",
        "            'å£²å´æç›Š': ['sum', 'count'],  # å£²å´æç›Šåˆè¨ˆã€å£²å´å›æ•°\n",
        "            'å„Ÿé‚„æç›Š': ['sum', 'count'],  # å„Ÿé‚„æç›Šåˆè¨ˆã€å„Ÿé‚„å›æ•°\n",
        "            'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å–å¼•ãƒ•ãƒ©ã‚°': 'mean',  # ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡\n",
        "            'å•†å“å': 'nunique',  # å•†å“å¤šæ§˜æ€§\n",
        "            'ã‚´ãƒ¼ãƒ«è¨­å®šå®Ÿæ–½': 'mean',  # ã‚´ãƒ¼ãƒ«è¨­å®šç‡\n",
        "            'ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šå®Ÿæ–½': 'mean'  # ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šç‡\n",
        "        }).fillna(0)\n",
        "\n",
        "        # ã‚«ãƒ©ãƒ åã‚’æ•´ç†\n",
        "        customer_stats.columns = [\n",
        "            'éå»å–å¼•å›æ•°', 'éå»ç·å–å¾—é¡', 'éå»å¹³å‡å–å¾—é¡',\n",
        "            'å£²å´æç›Šåˆè¨ˆ', 'å£²å´å›æ•°', 'å„Ÿé‚„æç›Šåˆè¨ˆ', 'å„Ÿé‚„å›æ•°',\n",
        "            'ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡', 'å•†å“å¤šæ§˜æ€§', 'ã‚´ãƒ¼ãƒ«è¨­å®šç‡', 'ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šç‡'\n",
        "        ]\n",
        "\n",
        "        # æ´¾ç”Ÿç‰¹å¾´é‡\n",
        "        customer_stats['éå»ç´¯ç©æç›Š'] = customer_stats['å£²å´æç›Šåˆè¨ˆ'] + customer_stats['å„Ÿé‚„æç›Šåˆè¨ˆ']\n",
        "        customer_stats['æŠ•è³‡æˆåŠŸä½“é¨“'] = (customer_stats['éå»ç´¯ç©æç›Š'] > 0).astype(int)\n",
        "        customer_stats['å¹³å‡åˆ©ç›Šç‡'] = customer_stats['éå»ç´¯ç©æç›Š'] / (customer_stats['éå»ç·å–å¾—é¡'] + 1)\n",
        "        customer_stats['ãƒªã‚¹ã‚¯ç®¡ç†åº¦'] = (customer_stats['ã‚´ãƒ¼ãƒ«è¨­å®šç‡'] + customer_stats['ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šç‡']) / 2\n",
        "\n",
        "        # æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—åˆ†é¡\n",
        "        customer_stats['æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'ãã®ä»–'\n",
        "\n",
        "        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–æŠ•è³‡å®¶\n",
        "        active_mask = (\n",
        "            (customer_stats['éå»å–å¼•å›æ•°'] > customer_stats['éå»å–å¼•å›æ•°'].quantile(0.7)) &\n",
        "            (customer_stats['æŠ•è³‡æˆåŠŸä½“é¨“'] == 1)\n",
        "        )\n",
        "        customer_stats.loc[active_mask, 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–'\n",
        "\n",
        "        # æ…é‡æŠ•è³‡å®¶\n",
        "        careful_mask = customer_stats['ãƒªã‚¹ã‚¯ç®¡ç†åº¦'] > 0.5\n",
        "        customer_stats.loc[careful_mask, 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'æ…é‡'\n",
        "\n",
        "        # ãƒ‡ã‚¸ã‚¿ãƒ«æŠ•è³‡å®¶\n",
        "        digital_mask = customer_stats['ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡'] > 0.7\n",
        "        customer_stats.loc[digital_mask, 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'ãƒ‡ã‚¸ã‚¿ãƒ«'\n",
        "\n",
        "        # å…ƒãƒ‡ãƒ¼ã‚¿ã«çµåˆ\n",
        "        df = df.merge(customer_stats, left_on='é¡§å®¢ID', right_index=True, how='left')\n",
        "\n",
        "        # æ¬ æå€¤å‡¦ç†\n",
        "        numeric_cols = [col for col in customer_stats.columns if col != 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—']\n",
        "        for col in numeric_cols:\n",
        "            df[col] = df[col].fillna(0)\n",
        "        df['æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = df['æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'].fillna('ãã®ä»–')\n",
        "\n",
        "        print(f\"   âœ… å–å¼•ç‰¹å¾´é‡ {len(customer_stats.columns)}å€‹ã‚’è¿½åŠ \")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ å–å¼•ç‰¹å¾´é‡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return add_dummy_features(df)"
      ],
      "metadata": {
        "id": "kSB2M-xmOROI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_real_product_features(df):\n",
        "    \"\"\"å®Ÿéš›ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
        "    try:\n",
        "        # å–å¼•ãƒ‡ãƒ¼ã‚¿ã¨å•†å“æƒ…å ±ã‚’çµåˆ\n",
        "        trading_with_product = trading_df.merge(\n",
        "            product_df[['å•†å“å', 'å•†å“ã‚«ãƒ†ã‚´ãƒª', 'ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'é€šè²¨', 'å‹§èª˜ç•™æ„å•†å“']],\n",
        "            on='å•†å“å', how='left'\n",
        "        )\n",
        "\n",
        "        print(f\"   å•†å“æƒ…å ±çµåˆå¾Œ: {trading_with_product.shape}\")\n",
        "\n",
        "        # é¡§å®¢åˆ¥å•†å“ç‰¹æ€§é›†è¨ˆ\n",
        "        customer_products = trading_with_product.groupby('é¡§å®¢ID').agg({\n",
        "            'å•†å“ã‚«ãƒ†ã‚´ãƒª': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'ãã®ä»–',\n",
        "            'ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Medium',\n",
        "            'é€šè²¨': lambda x: (x == 'JPY').mean(),  # å††å»ºã¦æ¯”ç‡\n",
        "            'å‹§èª˜ç•™æ„å•†å“': 'mean'  # ç•™æ„å•†å“æ¯”ç‡\n",
        "        })\n",
        "\n",
        "        customer_products.columns = ['ä¸»è¦å•†å“ã‚«ãƒ†ã‚´ãƒª', 'ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'å††å»ºã¦æ¯”ç‡', 'ç•™æ„å•†å“æ¯”ç‡']\n",
        "\n",
        "        # ãƒªã‚¹ã‚¯æŒ‡å‘åº¦ã‚¹ã‚³ã‚¢\n",
        "        risk_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
        "        customer_products['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'] = customer_products['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª'].map(risk_mapping).fillna(2)\n",
        "\n",
        "        # å…ƒãƒ‡ãƒ¼ã‚¿ã«çµåˆ\n",
        "        df = df.merge(customer_products, left_on='é¡§å®¢ID', right_index=True, how='left')\n",
        "\n",
        "        # æ¬ æå€¤å‡¦ç†\n",
        "        df['ä¸»è¦å•†å“ã‚«ãƒ†ã‚´ãƒª'] = df['ä¸»è¦å•†å“ã‚«ãƒ†ã‚´ãƒª'].fillna('ãã®ä»–')\n",
        "        df['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª'] = df['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª'].fillna('Medium')\n",
        "        df['å††å»ºã¦æ¯”ç‡'] = df['å††å»ºã¦æ¯”ç‡'].fillna(1.0)\n",
        "        df['ç•™æ„å•†å“æ¯”ç‡'] = df['ç•™æ„å•†å“æ¯”ç‡'].fillna(0.0)\n",
        "        df['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'] = df['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'].fillna(2)\n",
        "\n",
        "        print(f\"   âœ… å•†å“ç‰¹å¾´é‡ {len(customer_products.columns)}å€‹ã‚’è¿½åŠ \")\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ å•†å“ç‰¹å¾´é‡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "pIB2wLpDOd-T"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_dummy_features(df):\n",
        "    \"\"\"ãƒ€ãƒŸãƒ¼ç‰¹å¾´é‡è¿½åŠ \"\"\"\n",
        "    dummy_features = [\n",
        "        'éå»å–å¼•å›æ•°', 'éå»ç´¯ç©æç›Š', 'æŠ•è³‡æˆåŠŸä½“é¨“', 'ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡',\n",
        "        'å•†å“å¤šæ§˜æ€§', 'ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'\n",
        "    ]\n",
        "    for feature in dummy_features:\n",
        "        df[feature] = 0 if feature not in ['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] else 'ãã®ä»–'\n",
        "    return df"
      ],
      "metadata": {
        "id": "WVK5Oh1qKhHL"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"
      ],
      "metadata": {
        "id": "vezajMZ8KutT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PowerfulEnsemble:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'rf': RandomForestClassifier(\n",
        "                n_estimators=150,\n",
        "                max_depth=10,\n",
        "                min_samples_leaf=8,\n",
        "                max_features=0.8,\n",
        "                random_state=42,\n",
        "                n_jobs=-1,\n",
        "                class_weight='balanced'\n",
        "            ),\n",
        "            'et': ExtraTreesClassifier(\n",
        "                n_estimators=150,\n",
        "                max_depth=12,\n",
        "                min_samples_leaf=6,\n",
        "                max_features=0.9,\n",
        "                random_state=42,\n",
        "                n_jobs=-1,\n",
        "                class_weight='balanced'\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y, term_id):\n",
        "        print(f\"\\nğŸ¯ Term {term_id} - å¼·åŒ–å­¦ç¿’\")\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿: {X.shape}, é™½æ€§ç‡: {y.mean():.4f}\")\n",
        "\n",
        "        cv_scores = {}\n",
        "        for name, model in self.models.items():\n",
        "            model.fit(X, y)\n",
        "            cv_score = cross_val_score(model, X, y, cv=3, scoring='roc_auc')\n",
        "            cv_scores[name] = cv_score.mean()\n",
        "            print(f\"  {name}: {cv_score.mean():.4f} (Â±{cv_score.std():.3f})\")\n",
        "\n",
        "        # æ€§èƒ½ãƒ™ãƒ¼ã‚¹é‡ã¿ä»˜ã‘\n",
        "        total_score = sum(cv_scores.values())\n",
        "        self.weights = {name: score/total_score for name, score in cv_scores.items()}\n",
        "        print(f\"  é‡ã¿: {self.weights}\")\n",
        "\n",
        "        return cv_scores\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        predictions = []\n",
        "        weights = []\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            pred = model.predict_proba(X)[:, 1]\n",
        "            predictions.append(pred)\n",
        "            weights.append(self.weights[name])\n",
        "\n",
        "        # é‡ã¿ä»˜ãã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«\n",
        "        ensemble_pred = sum(w * p for w, p in zip(weights, predictions))\n",
        "        return ensemble_pred\n",
        "\n",
        "def complete_pipeline(train_df, test_df):\n",
        "    \"\"\"å®Œå…¨ç‰ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\"\"\"\n",
        "    print(\"ğŸš€ å®Œå…¨ç‰ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # å®Œå…¨ç‰¹å¾´é‡ä½œæˆ\n",
        "    train_complete = create_complete_features(train_df.copy())\n",
        "    test_complete = create_complete_features(test_df.copy())\n",
        "\n",
        "    # æœ€å¼·ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ\n",
        "    power_features = [\n",
        "        # åŸºæœ¬é‡è¦ç‰¹å¾´é‡\n",
        "        'æ™‚ä¾¡ä¾¡é¡', 'é¡§å®¢ãƒ©ãƒ³ã‚¯', 'å¹´é½¢Ã—è³‡ç”£è¦æ¨¡', 'éå»å–å¼•å›æ•°',\n",
        "        'è³‡ç”£è¦æ¨¡', 'æŠ•è³‡åŠ¹ç‡', 'å¹´é½¢èª¿æ•´è³‡ç”£', 'å«ã¿æç›Šç‡',\n",
        "\n",
        "        # å®Ÿãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡\n",
        "        'å•†å“å¤šæ§˜æ€§', 'ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡', 'æŠ•è³‡æˆåŠŸä½“é¨“', 'éå»ç´¯ç©æç›Š',\n",
        "        'ãƒªã‚¹ã‚¯ç®¡ç†åº¦', 'ãƒªã‚¹ã‚¯æŒ‡å‘åº¦', 'å††å»ºã¦æ¯”ç‡',\n",
        "\n",
        "        # åŸºæœ¬æƒ…å ±\n",
        "        'é¡§å®¢å¹´é½¢', 'æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰', 'year', 'æŠ•è³‡æ–¹é‡',\n",
        "\n",
        "        # ãƒ•ãƒ©ã‚°\n",
        "        'é«˜è³‡ç”£ãƒ•ãƒ©ã‚°', 'å«ã¿æãƒ•ãƒ©ã‚°', 'ã‚·ãƒ‹ã‚¢ãƒ•ãƒ©ã‚°', 'æ ªå¼çµŒé¨“ã‚ã‚Š'\n",
        "    ]\n",
        "\n",
        "    print(f\"ğŸ“Š å¼·åŒ–ç‰¹å¾´é‡: {len(power_features)}å€‹\")\n",
        "\n",
        "    # Termåˆ¥äºˆæ¸¬\n",
        "    all_predictions = pd.DataFrame()\n",
        "    term_aucs = []\n",
        "\n",
        "    for term_id in range(1, 7):\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Term {term_id} å¼·åŒ–å‡¦ç†\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
        "        train_term = train_complete[train_complete[f'train_term_{term_id}'] == 1]\n",
        "        test_term = test_complete[test_complete[f'test_term_{term_id}'] == 1]\n",
        "\n",
        "        # åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡\n",
        "        available_features = [f for f in power_features if f in train_term.columns]\n",
        "        print(f\"åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {len(available_features)}å€‹\")\n",
        "\n",
        "        X_train = train_term[available_features].fillna(0)\n",
        "        y_train = train_term['y']\n",
        "        X_test = test_term[available_features].fillna(0)\n",
        "\n",
        "        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
        "        categorical_cols = ['æŠ•è³‡æ–¹é‡', 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—', 'ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª']\n",
        "        categorical_to_encode = [col for col in categorical_cols if col in X_train.columns]\n",
        "\n",
        "        if categorical_to_encode:\n",
        "            X_train = pd.get_dummies(X_train, columns=categorical_to_encode, drop_first=True, dtype=int)\n",
        "            X_test = pd.get_dummies(X_test, columns=categorical_to_encode, drop_first=True, dtype=int)\n",
        "\n",
        "            # ã‚«ãƒ©ãƒ èª¿æ•´\n",
        "            missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "            for col in missing_cols:\n",
        "                X_test[col] = 0\n",
        "            X_test = X_test[X_train.columns]\n",
        "\n",
        "        print(f\"æœ€çµ‚ç‰¹å¾´é‡æ•°: {X_train.shape[1]}å€‹\")\n",
        "\n",
        "        # å¼·åŒ–ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
        "        ensemble = PowerfulEnsemble()\n",
        "        cv_scores = ensemble.fit(X_train, y_train, term_id)\n",
        "        term_aucs.append(np.mean(list(cv_scores.values())))\n",
        "\n",
        "        # äºˆæ¸¬\n",
        "        test_pred = ensemble.predict_proba(X_test)\n",
        "\n",
        "        # çµæœä¿å­˜\n",
        "        pred_df = pd.DataFrame({'ID': test_term['ID'], 'predict': test_pred})\n",
        "        all_predictions = pd.concat([all_predictions, pred_df]) if not all_predictions.empty else pred_df\n",
        "\n",
        "    # æœ€çµ‚çµæœ\n",
        "    avg_auc = np.mean(term_aucs)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ğŸ‰ å®Œå…¨ç‰ˆçµæœ\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for i, auc in enumerate(term_aucs, 1):\n",
        "        print(f\"Term {i}: {auc:.4f}\")\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ å®Œå…¨ç‰ˆå¹³å‡AUC: {avg_auc:.4f}\")\n",
        "    print(f\"ğŸ“Š æ”¹å–„å¹…: +{avg_auc-0.62:.3f}\")\n",
        "\n",
        "    if avg_auc >= 0.67:\n",
        "        print(\"ğŸ‰ å¤§æˆåŠŸï¼ç›®æ¨™å¤§å¹…é”æˆï¼\")\n",
        "    elif avg_auc >= 0.65:\n",
        "        print(\"âœ… ç›®æ¨™é”æˆï¼\")\n",
        "    elif avg_auc >= 0.64:\n",
        "        print(\"ğŸ“ˆ å¤§å¹…æ”¹å–„ï¼\")\n",
        "\n",
        "    return all_predictions, avg_auc"
      ],
      "metadata": {
        "id": "C5wDNgCXKr6U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®å¼·åŒ–"
      ],
      "metadata": {
        "id": "iRopblc5K2pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–¢æ•°\n",
        "def evaluate_model_performance(y_true, y_pred_proba, threshold=0.23):\n",
        "    \"\"\"PoCã§è¨­å®šã—ãŸæŒ‡æ¨™ã§è©•ä¾¡\"\"\"\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        'AUC': roc_auc_score(y_true, y_pred_proba),\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'F1': f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"AUC: {metrics['AUC']:.4f} (ç›®æ¨™: 0.7ä»¥ä¸Š)\")\n",
        "    print(f\"Precision: {metrics['Precision']:.4f} (ç›®æ¨™: 0.4ä»¥ä¸Š)\")\n",
        "    print(f\"Recall: {metrics['Recall']:.4f} (ç›®æ¨™: 0.6ä»¥ä¸Š)\")\n",
        "    print(f\"F1 Score: {metrics['F1']:.4f} (ç›®æ¨™: 0.5ä»¥ä¸Š)\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "sgDwaXdXK1pl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰"
      ],
      "metadata": {
        "id": "-UEuTjAjK-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸš€ å®Œå…¨ç‰ˆå®Ÿè¡Œé–‹å§‹\")\n",
        "complete_predictions, complete_auc = complete_pipeline(train_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfl0gesBLDg_",
        "outputId": "6907eb74-5fd5-49f0-fb86-da1d6fb9da67"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ å®Œå…¨ç‰ˆå®Ÿè¡Œé–‹å§‹\n",
            "ğŸš€ å®Œå…¨ç‰ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹\n",
            "============================================================\n",
            "ğŸš€ å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆé–‹å§‹\n",
            "ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¼•ç‰¹å¾´é‡ä½œæˆ\n",
            "   éå»å–å¼•ãƒ‡ãƒ¼ã‚¿: (31638, 20)\n",
            "   âœ… å–å¼•ç‰¹å¾´é‡ 16å€‹ã‚’è¿½åŠ \n",
            "ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å•†å“ç‰¹å¾´é‡ä½œæˆ\n",
            "   å•†å“æƒ…å ±çµåˆå¾Œ: (37514, 24)\n",
            "   âœ… å•†å“ç‰¹å¾´é‡ 5å€‹ã‚’è¿½åŠ \n",
            "âœ… å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆå®Œäº†\n",
            "ğŸš€ å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆé–‹å§‹\n",
            "ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å–å¼•ç‰¹å¾´é‡ä½œæˆ\n",
            "   éå»å–å¼•ãƒ‡ãƒ¼ã‚¿: (31638, 20)\n",
            "   âœ… å–å¼•ç‰¹å¾´é‡ 16å€‹ã‚’è¿½åŠ \n",
            "ğŸ“Š å®Ÿãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å•†å“ç‰¹å¾´é‡ä½œæˆ\n",
            "   å•†å“æƒ…å ±çµåˆå¾Œ: (37514, 24)\n",
            "   âœ… å•†å“ç‰¹å¾´é‡ 5å€‹ã‚’è¿½åŠ \n",
            "âœ… å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆå®Œäº†\n",
            "ğŸ“Š å¼·åŒ–ç‰¹å¾´é‡: 23å€‹\n",
            "\n",
            "========================================\n",
            "Term 1 å¼·åŒ–å‡¦ç†\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 23å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 24å€‹\n",
            "\n",
            "ğŸ¯ Term 1 - å¼·åŒ–å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (80000, 24), é™½æ€§ç‡: 0.1972\n",
            "  rf: 0.6729 (Â±0.014)\n",
            "  et: 0.6685 (Â±0.012)\n",
            "  é‡ã¿: {'rf': np.float64(0.5016413006201333), 'et': np.float64(0.49835869937986677)}\n",
            "\n",
            "========================================\n",
            "Term 2 å¼·åŒ–å‡¦ç†\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 23å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 24å€‹\n",
            "\n",
            "ğŸ¯ Term 2 - å¼·åŒ–å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (82000, 24), é™½æ€§ç‡: 0.1976\n",
            "  rf: 0.6761 (Â±0.009)\n",
            "  et: 0.6732 (Â±0.007)\n",
            "  é‡ã¿: {'rf': np.float64(0.5010882775008142), 'et': np.float64(0.4989117224991858)}\n",
            "\n",
            "========================================\n",
            "Term 3 å¼·åŒ–å‡¦ç†\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 23å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 24å€‹\n",
            "\n",
            "ğŸ¯ Term 3 - å¼·åŒ–å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (84000, 24), é™½æ€§ç‡: 0.1980\n",
            "  rf: 0.6689 (Â±0.020)\n",
            "  et: 0.6701 (Â±0.007)\n",
            "  é‡ã¿: {'rf': np.float64(0.4995348280554053), 'et': np.float64(0.5004651719445947)}\n",
            "\n",
            "========================================\n",
            "Term 4 å¼·åŒ–å‡¦ç†\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 23å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 24å€‹\n",
            "\n",
            "ğŸ¯ Term 4 - å¼·åŒ–å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (86000, 24), é™½æ€§ç‡: 0.1985\n",
            "  rf: 0.6740 (Â±0.018)\n",
            "  et: 0.6699 (Â±0.012)\n",
            "  é‡ã¿: {'rf': np.float64(0.5015078540229916), 'et': np.float64(0.4984921459770084)}\n",
            "\n",
            "========================================\n",
            "Term 5 å¼·åŒ–å‡¦ç†\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 23å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 24å€‹\n",
            "\n",
            "ğŸ¯ Term 5 - å¼·åŒ–å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (88000, 24), é™½æ€§ç‡: 0.1989\n",
            "  rf: 0.6642 (Â±0.019)\n",
            "  et: 0.6630 (Â±0.017)\n",
            "  é‡ã¿: {'rf': np.float64(0.5004654785796909), 'et': np.float64(0.499534521420309)}\n",
            "\n",
            "========================================\n",
            "Term 6 å¼·åŒ–å‡¦ç†\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 23å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 24å€‹\n",
            "\n",
            "ğŸ¯ Term 6 - å¼·åŒ–å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (90000, 24), é™½æ€§ç‡: 0.1989\n",
            "  rf: 0.6473 (Â±0.022)\n",
            "  et: 0.6517 (Â±0.011)\n",
            "  é‡ã¿: {'rf': np.float64(0.4983141347047995), 'et': np.float64(0.5016858652952004)}\n",
            "\n",
            "============================================================\n",
            "ğŸ‰ å®Œå…¨ç‰ˆçµæœ\n",
            "============================================================\n",
            "Term 1: 0.6707\n",
            "Term 2: 0.6746\n",
            "Term 3: 0.6695\n",
            "Term 4: 0.6720\n",
            "Term 5: 0.6636\n",
            "Term 6: 0.6495\n",
            "\n",
            "ğŸ“ˆ å®Œå…¨ç‰ˆå¹³å‡AUC: 0.6667\n",
            "ğŸ“Š æ”¹å–„å¹…: +0.047\n",
            "âœ… ç›®æ¨™é”æˆï¼\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission_df[1] = complete_predictions['predict']\n",
        "submission_df.to_csv(base_dir + '/complete_submission.csv', index=False, header=False)\n",
        "\n",
        "print(f\"\\nğŸ‰ å®Œå…¨ç‰ˆå®Œäº†ï¼\")\n",
        "print(f\"æœ€çµ‚AUC: {complete_auc:.4f}\")\n",
        "print(f\"æ”¹å–„: 0.62 â†’ {complete_auc:.4f} (+{complete_auc-0.62:.3f})\")\n",
        "print(\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: complete_submission.csv\")"
      ],
      "metadata": {
        "id": "GMqCAoZVPyUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5749bc0-0455-46f0-99e0-389ea9e7d61c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ‰ å®Œå…¨ç‰ˆå®Œäº†ï¼\n",
            "æœ€çµ‚AUC: 0.6667\n",
            "æ”¹å–„: 0.62 â†’ 0.6667 (+0.047)\n",
            "æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: complete_submission.csv\n"
          ]
        }
      ]
    }
  ]
}