{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyPuem8/XMRaTTeK1crz3U7K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hassaku12/manabiDX2025/blob/main/%E6%BC%94%E7%BF%9203%EF%BC%88%E8%A9%A6%E3%81%99%E7%94%A8%EF%BC%89.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿å–ã‚Š"
      ],
      "metadata": {
        "id": "5RWmI06ynebb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "V2u0gVJvouG_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "188ca0c1-4c44-484f-ac34-c016c25f6cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# ã¾ãšã¯Googleãƒ‰ãƒ©ã‚¤ãƒ–ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# japanize-matplotlibã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« (å®Ÿè¡Œç’°å¢ƒã«æœªå°å…¥ã®å ´åˆ)\n",
        "!pip install japanize-matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCJXCkY4EUNg",
        "outputId": "bb9dada3-6b86-4848-a17f-30287cb55121"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting japanize-matplotlib\n",
            "  Downloading japanize-matplotlib-1.1.3.tar.gz (4.1 MB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/4.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m181.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from japanize-matplotlib) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->japanize-matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->japanize-matplotlib) (1.17.0)\n",
            "Building wheels for collected packages: japanize-matplotlib\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for japanize-matplotlib: filename=japanize_matplotlib-1.1.3-py3-none-any.whl size=4120257 sha256=7707fa96c61a2f263da1e435419d89b95954246990d71e7b1ed4d4a7a59cec8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/f7/9b/418f19a7b9340fc16e071e89efc379aca68d40238b258df53d\n",
            "Successfully built japanize-matplotlib\n",
            "Installing collected packages: japanize-matplotlib\n",
            "Successfully installed japanize-matplotlib-1.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import japanize_matplotlib\n",
        "import seaborn as sns\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "o6mDl66NEpqI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/Colab Notebooks/ãƒãƒŠãƒ’ã‚™DX'"
      ],
      "metadata": {
        "id": "PB_duKNHFzL9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "train_df = pd.read_csv(base_dir + '/train.csv')\n",
        "test_df = pd.read_csv(base_dir + '/test.csv')\n",
        "submission_df = pd.read_csv(base_dir + '/sample_submit.csv', header=None)\n",
        "assessment_df = pd.read_csv(base_dir + '/é©åˆæ€§åˆ¤å®šã‚·ãƒ¼ãƒˆä¸€è¦§è¡¨.csv')"
      ],
      "metadata": {
        "id": "T2oQ0Gv7GEQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93968ef2-43de-4077-cc2d-1a3c1a493ff3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ“Š è¿½åŠ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\")\n",
        "\n",
        "# ç´„å®šãƒ‡ãƒ¼ã‚¿ã¨å•†å“ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿\n",
        "try:\n",
        "    trading_df = pd.read_csv(base_dir + '/ç´„å®šãƒ‡ãƒ¼ã‚¿ä¸€è¦§è¡¨.csv')\n",
        "    product_df = pd.read_csv(base_dir + '/å•†å“ãƒªã‚¹ãƒˆ.csv')\n",
        "\n",
        "    print(f\"âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿: {trading_df.shape}\")\n",
        "    print(f\"âœ… å•†å“ãƒªã‚¹ãƒˆ: {product_df.shape}\")\n",
        "    print(f\"âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿åˆ—: {list(trading_df.columns)}\")\n",
        "    print(f\"âœ… å•†å“ãƒªã‚¹ãƒˆåˆ—: {list(product_df.columns)}\")\n",
        "\n",
        "    HAS_ALL_DATA = True\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"âŒ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "    HAS_ALL_DATA = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2E5vkA4W3Wu3",
        "outputId": "1a430186-4c84-4af6-e873-fb7359c7d7b4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š è¿½åŠ ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­...\n",
            "âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿: (37514, 20)\n",
            "âœ… å•†å“ãƒªã‚¹ãƒˆ: (786, 18)\n",
            "âœ… ç´„å®šãƒ‡ãƒ¼ã‚¿åˆ—: ['å–å¼•æ—¥', 'é¡§å®¢ID', 'å–å¼•ã‚³ãƒ¼ãƒ‰', 'å•†å“å', 'å–å¾—ä¾¡é¡', 'å–å¾—å˜ä¾¡', 'å–å¼•æ•°é‡', 'å£²å´å˜ä¾¡', 'å£²å´ä¾¡é¡', 'å£²å´æç›Š', 'å„Ÿé‚„ä¾¡é¡', 'å„Ÿé‚„å˜ä¾¡', 'å„Ÿé‚„æç›Š', 'ç©ç«‹æŠ•è³‡è³¼å…¥ã®æ–°è¦/æ—¢å­˜', 'è³¼å…¥é–‹å§‹å¹´æœˆ', 'è³¼å…¥çµ‚äº†å¹´æœˆ', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å–å¼•ãƒ•ãƒ©ã‚°', 'ã‚´ãƒ¼ãƒ«è¨­å®šå®Ÿæ–½', 'ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šå®Ÿæ–½', 'ãƒ­ã‚¹ã‚«ãƒƒãƒˆæ°´æº–']\n",
            "âœ… å•†å“ãƒªã‚¹ãƒˆåˆ—: ['å•†å“å', 'å•†å“ã‚«ãƒ†ã‚´ãƒª', 'ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'é€šè²¨', 'å‹§èª˜ç•™æ„å•†å“', 'è§£ç´„æ‰‹æ•°æ–™ç‡', 'ç™ºè¡Œæ—¥', 'å„Ÿé‚„æ—¥', 'åˆ©ç‡', 'ç™ºè¡Œæ—¥æ ªä¾¡', 'æ—©æœŸå„Ÿé‚„æ ªä¾¡', 'ãƒãƒƒã‚¯ã‚¤ãƒ³æ ªä¾¡', 'æ—©æœŸå„Ÿé‚„æ—¥', 'ãƒãƒƒã‚¯ã‚¤ãƒ³æ—¥', 'å¥‘ç´„å¹´æœˆ', 'äºˆå®šåˆ©ç‡', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ°é”æ—¥']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†"
      ],
      "metadata": {
        "id": "AhZBUc-aJyJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# é¡§å®¢å±æ€§ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
        "assessment_df['å–å¼•æ—¥'] = pd.to_datetime(assessment_df['å–å¼•æ—¥'])\n",
        "assessment_df_latest = assessment_df.sort_values(['é¡§å®¢ID', 'å–å¼•æ—¥']).drop_duplicates(subset='é¡§å®¢ID', keep='last')\n",
        "assessment_df_selected = assessment_df_latest[['é¡§å®¢ID', 'é¡§å®¢å¹´é½¢', 'æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰']]"
      ],
      "metadata": {
        "id": "geqgG3BxJ0I6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# é¡§å®¢å±æ€§æƒ…å ±ã‚’çµåˆ\n",
        "train_df = pd.merge(train_df, assessment_df_selected, on='é¡§å®¢ID', how='left')\n",
        "test_df = pd.merge(test_df, assessment_df_selected, on='é¡§å®¢ID', how='left')"
      ],
      "metadata": {
        "id": "xs6QOxbEKTNB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# æ¬ æå€¤ã‚’è£œå®Œ\n",
        "train_df['é¡§å®¢å¹´é½¢'] = train_df['é¡§å®¢å¹´é½¢'].fillna(train_df['é¡§å®¢å¹´é½¢'].mean())\n",
        "test_df['é¡§å®¢å¹´é½¢'] = test_df['é¡§å®¢å¹´é½¢'].fillna(test_df['é¡§å®¢å¹´é½¢'].mean())\n",
        "train_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'] = train_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'].fillna(0)\n",
        "test_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'] = test_df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'].fillna(0)"
      ],
      "metadata": {
        "id": "TheHXUiyKVQm"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# åŸºæº–å¹´æœˆã‹ã‚‰å¹´ã¨æœˆã‚’æŠ½å‡º\n",
        "train_df['year'] = train_df['åŸºæº–å¹´æœˆ'].str.split('-').str[0].astype(int)\n",
        "train_df['month'] = train_df['åŸºæº–å¹´æœˆ'].str.split('-').str[1].astype(int)\n",
        "test_df['year'] = test_df['åŸºæº–å¹´æœˆ'].str.split('-').str[0].astype(int)\n",
        "test_df['month'] = test_df['åŸºæº–å¹´æœˆ'].str.split('-').str[1].astype(int)"
      ],
      "metadata": {
        "id": "cwCZR8nTKYEw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆã™ã¹ã¦ã‚’é–¢æ•°ã§çµ±ä¸€ï¼‰"
      ],
      "metadata": {
        "id": "bFWAxiA072mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# æ—¢å­˜ã®é–¢æ•°ç¾¤ï¼ˆå¿…é ˆï¼‰\n",
        "def add_real_trading_features(df):\n",
        "    \"\"\"å®Ÿéš›ã®ç´„å®šãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
        "    try:\n",
        "        trading_df['å–å¼•æ—¥'] = pd.to_datetime(trading_df['å–å¼•æ—¥'])\n",
        "        past_trading = trading_df[trading_df['å–å¼•æ—¥'] <= '2021-11-30']\n",
        "\n",
        "        customer_stats = past_trading.groupby('é¡§å®¢ID').agg({\n",
        "            'å–å¾—ä¾¡é¡': ['count', 'sum', 'mean'],\n",
        "            'å£²å´æç›Š': ['sum', 'count'],\n",
        "            'å„Ÿé‚„æç›Š': ['sum', 'count'],\n",
        "            'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å–å¼•ãƒ•ãƒ©ã‚°': 'mean',\n",
        "            'å•†å“å': 'nunique',\n",
        "            'ã‚´ãƒ¼ãƒ«è¨­å®šå®Ÿæ–½': 'mean',\n",
        "            'ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šå®Ÿæ–½': 'mean'\n",
        "        }).fillna(0)\n",
        "\n",
        "        customer_stats.columns = [\n",
        "            'éå»å–å¼•å›æ•°', 'éå»ç·å–å¾—é¡', 'éå»å¹³å‡å–å¾—é¡',\n",
        "            'å£²å´æç›Šåˆè¨ˆ', 'å£²å´å›æ•°', 'å„Ÿé‚„æç›Šåˆè¨ˆ', 'å„Ÿé‚„å›æ•°',\n",
        "            'ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡', 'å•†å“å¤šæ§˜æ€§', 'ã‚´ãƒ¼ãƒ«è¨­å®šç‡', 'ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šç‡'\n",
        "        ]\n",
        "\n",
        "        customer_stats['éå»ç´¯ç©æç›Š'] = customer_stats['å£²å´æç›Šåˆè¨ˆ'] + customer_stats['å„Ÿé‚„æç›Šåˆè¨ˆ']\n",
        "        customer_stats['æŠ•è³‡æˆåŠŸä½“é¨“'] = (customer_stats['éå»ç´¯ç©æç›Š'] > 0).astype(int)\n",
        "        customer_stats['å¹³å‡åˆ©ç›Šç‡'] = customer_stats['éå»ç´¯ç©æç›Š'] / (customer_stats['éå»ç·å–å¾—é¡'] + 1)\n",
        "        customer_stats['ãƒªã‚¹ã‚¯ç®¡ç†åº¦'] = (customer_stats['ã‚´ãƒ¼ãƒ«è¨­å®šç‡'] + customer_stats['ãƒ­ã‚¹ã‚«ãƒƒãƒˆè¨­å®šç‡']) / 2\n",
        "\n",
        "        # æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—åˆ†é¡\n",
        "        customer_stats['æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'ãã®ä»–'\n",
        "        active_mask = (\n",
        "            (customer_stats['éå»å–å¼•å›æ•°'] > customer_stats['éå»å–å¼•å›æ•°'].quantile(0.7)) &\n",
        "            (customer_stats['æŠ•è³‡æˆåŠŸä½“é¨“'] == 1)\n",
        "        )\n",
        "        customer_stats.loc[active_mask, 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'ã‚¢ã‚¯ãƒ†ã‚£ãƒ–'\n",
        "\n",
        "        careful_mask = customer_stats['ãƒªã‚¹ã‚¯ç®¡ç†åº¦'] > 0.5\n",
        "        customer_stats.loc[careful_mask, 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'æ…é‡'\n",
        "\n",
        "        digital_mask = customer_stats['ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡'] > 0.7\n",
        "        customer_stats.loc[digital_mask, 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = 'ãƒ‡ã‚¸ã‚¿ãƒ«'\n",
        "\n",
        "        df = df.merge(customer_stats, left_on='é¡§å®¢ID', right_index=True, how='left')\n",
        "\n",
        "        numeric_cols = [col for col in customer_stats.columns if col != 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—']\n",
        "        for col in numeric_cols:\n",
        "            df[col] = df[col].fillna(0)\n",
        "        df['æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'] = df['æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—'].fillna('ãã®ä»–')\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"å–å¼•ç‰¹å¾´é‡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return add_dummy_features(df)"
      ],
      "metadata": {
        "id": "1JyaHnU28D3n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_real_product_features(df):\n",
        "    \"\"\"å®Ÿéš›ã®å•†å“ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
        "    try:\n",
        "        trading_with_product = trading_df.merge(\n",
        "            product_df[['å•†å“å', 'å•†å“ã‚«ãƒ†ã‚´ãƒª', 'ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'é€šè²¨', 'å‹§èª˜ç•™æ„å•†å“']],\n",
        "            on='å•†å“å', how='left'\n",
        "        )\n",
        "\n",
        "        customer_products = trading_with_product.groupby('é¡§å®¢ID').agg({\n",
        "            'å•†å“ã‚«ãƒ†ã‚´ãƒª': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'ãã®ä»–',\n",
        "            'ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 else 'Medium',\n",
        "            'é€šè²¨': lambda x: (x == 'JPY').mean(),\n",
        "            'å‹§èª˜ç•™æ„å•†å“': 'mean'\n",
        "        })\n",
        "\n",
        "        customer_products.columns = ['ä¸»è¦å•†å“ã‚«ãƒ†ã‚´ãƒª', 'ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'å††å»ºã¦æ¯”ç‡', 'ç•™æ„å•†å“æ¯”ç‡']\n",
        "\n",
        "        risk_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
        "        customer_products['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'] = customer_products['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª'].map(risk_mapping).fillna(2)\n",
        "\n",
        "        df = df.merge(customer_products, left_on='é¡§å®¢ID', right_index=True, how='left')\n",
        "\n",
        "        df['ä¸»è¦å•†å“ã‚«ãƒ†ã‚´ãƒª'] = df['ä¸»è¦å•†å“ã‚«ãƒ†ã‚´ãƒª'].fillna('ãã®ä»–')\n",
        "        df['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª'] = df['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª'].fillna('Medium')\n",
        "        df['å††å»ºã¦æ¯”ç‡'] = df['å††å»ºã¦æ¯”ç‡'].fillna(1.0)\n",
        "        df['ç•™æ„å•†å“æ¯”ç‡'] = df['ç•™æ„å•†å“æ¯”ç‡'].fillna(0.0)\n",
        "        df['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'] = df['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'].fillna(2)\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"å•†å“ç‰¹å¾´é‡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
        "        return df"
      ],
      "metadata": {
        "id": "yBGxyllF8HOe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_dummy_features(df):\n",
        "    \"\"\"ãƒ€ãƒŸãƒ¼ç‰¹å¾´é‡è¿½åŠ \"\"\"\n",
        "    dummy_features = [\n",
        "        'éå»å–å¼•å›æ•°', 'éå»ç´¯ç©æç›Š', 'æŠ•è³‡æˆåŠŸä½“é¨“', 'ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡',\n",
        "        'å•†å“å¤šæ§˜æ€§', 'ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—', 'ãƒªã‚¹ã‚¯ç®¡ç†åº¦', 'ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'\n",
        "    ]\n",
        "    for feature in dummy_features:\n",
        "        if feature not in ['ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª', 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—']:\n",
        "            df[feature] = 0\n",
        "        else:\n",
        "            df[feature] = 'ãã®ä»–' if feature == 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—' else 'Medium'\n",
        "    df['å††å»ºã¦æ¯”ç‡'] = 1.0\n",
        "    df['ç•™æ„å•†å“æ¯”ç‡'] = 0.0\n",
        "    return df"
      ],
      "metadata": {
        "id": "bMAYKGeE8NpP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_complete_features(df):\n",
        "    \"\"\"å®Œå…¨ç‰ˆç‰¹å¾´é‡ä½œæˆ\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # åŸºæœ¬ç‰¹å¾´é‡\n",
        "    df['è³‡ç”£è¦æ¨¡'] = df['å–å¾—ä¾¡é¡'] + df['æ™‚ä¾¡ä¾¡é¡']\n",
        "    df['æŠ•è³‡åŠ¹ç‡'] = df['æ™‚ä¾¡ä¾¡é¡'] / (df['å–å¾—ä¾¡é¡'] + 1)\n",
        "    df['å¹´é½¢èª¿æ•´è³‡ç”£'] = df['æ™‚ä¾¡ä¾¡é¡'] / (df['é¡§å®¢å¹´é½¢'] + 1)\n",
        "    df['é¡§å®¢ãƒ©ãƒ³ã‚¯'] = df['æ™‚ä¾¡ä¾¡é¡'].rank(pct=True)\n",
        "    df['å¹´é½¢Ã—è³‡ç”£è¦æ¨¡'] = df['é¡§å®¢å¹´é½¢'] * df['è³‡ç”£è¦æ¨¡']\n",
        "    df['å«ã¿æç›Šç‡'] = np.where(df['å–å¾—ä¾¡é¡'] != 0,\n",
        "                                (df['æ™‚ä¾¡ä¾¡é¡'] - df['å–å¾—ä¾¡é¡']) / df['å–å¾—ä¾¡é¡'], 0)\n",
        "\n",
        "    if HAS_ALL_DATA:\n",
        "        df = add_real_trading_features(df)\n",
        "        df = add_real_product_features(df)\n",
        "    else:\n",
        "        df = add_dummy_features(df)\n",
        "\n",
        "    # ãƒ•ãƒ©ã‚°ç‰¹å¾´é‡\n",
        "    df['é«˜è³‡ç”£ãƒ•ãƒ©ã‚°'] = (df['è³‡ç”£è¦æ¨¡'] > df['è³‡ç”£è¦æ¨¡'].quantile(0.8)).astype(int)\n",
        "    df['å«ã¿æãƒ•ãƒ©ã‚°'] = (df['è©•ä¾¡æç›Š'] < 0).astype(int)\n",
        "    df['ã‚·ãƒ‹ã‚¢ãƒ•ãƒ©ã‚°'] = (df['é¡§å®¢å¹´é½¢'] >= 65).astype(int)\n",
        "    df['æ ªå¼çµŒé¨“ã‚ã‚Š'] = (df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰'] > 0).astype(int)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "aOsk1DKj8TBZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸš€ æœ€çµ‚èª¿æ•´ï¼šæ–°è¦ç‰¹å¾´é‡è¿½åŠ \n",
        "def create_final_features(df):\n",
        "    \"\"\"æœ€å¾Œã®+0.01ã‚’ç‹™ã†ç‰¹å¾´é‡\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # æ—¢å­˜ç‰¹å¾´é‡ä½œæˆ\n",
        "    df = create_complete_features(df)\n",
        "\n",
        "    # ã€æ–°è¦ã€‘é«˜åº¦ãªç›¸äº’ä½œç”¨ç‰¹å¾´é‡\n",
        "    df['æŠ•è³‡åŠ¹ç‡Ã—å¹´é½¢'] = df['æŠ•è³‡åŠ¹ç‡'] * df['é¡§å®¢å¹´é½¢']\n",
        "    df['ãƒªã‚¹ã‚¯Ã—çµŒé¨“'] = df['ãƒªã‚¹ã‚¯æŒ‡å‘åº¦'] * df['æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰']\n",
        "    df['è³‡ç”£Ã—å–å¼•é »åº¦'] = df['è³‡ç”£è¦æ¨¡'] * df['éå»å–å¼•å›æ•°']\n",
        "\n",
        "    # ã€æ–°è¦ã€‘æ¯”ç‡ç‰¹å¾´é‡\n",
        "    df['æˆåŠŸç‡'] = df['æŠ•è³‡æˆåŠŸä½“é¨“'] / (df['éå»å–å¼•å›æ•°'] + 1)\n",
        "    df['åŠ¹ç‡ãƒ©ãƒ³ã‚¯'] = df['æŠ•è³‡åŠ¹ç‡'].rank(pct=True)\n",
        "    df['å¹´é½¢å†…è³‡ç”£é †ä½'] = df.groupby('é¡§å®¢å¹´é½¢')['æ™‚ä¾¡ä¾¡é¡'].rank(pct=True)\n",
        "\n",
        "    # ã€æ–°è¦ã€‘è¤‡åˆã‚¹ã‚³ã‚¢\n",
        "    df['ç·åˆæŠ•è³‡åŠ›'] = (\n",
        "        df['é¡§å®¢ãƒ©ãƒ³ã‚¯'] * 0.3 +\n",
        "        df['åŠ¹ç‡ãƒ©ãƒ³ã‚¯'] * 0.3 +\n",
        "        df['æŠ•è³‡æˆåŠŸä½“é¨“'] * 0.2 +\n",
        "        (df['éå»å–å¼•å›æ•°'] / 50).clip(0, 1) * 0.2\n",
        "    )\n",
        "\n",
        "    # ã€æ–°è¦ã€‘ç•°å¸¸å€¤ãƒ•ãƒ©ã‚°\n",
        "    df['ç•°å¸¸é«˜è³‡ç”£'] = (df['æ™‚ä¾¡ä¾¡é¡'] > df['æ™‚ä¾¡ä¾¡é¡'].quantile(0.99)).astype(int)\n",
        "    df['è¶…ã‚¢ã‚¯ãƒ†ã‚£ãƒ–'] = (df['éå»å–å¼•å›æ•°'] > df['éå»å–å¼•å›æ•°'].quantile(0.95)).astype(int)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "ME9SbWxt8Uhp"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«"
      ],
      "metadata": {
        "id": "vezajMZ8KutT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ğŸ¯ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«\n",
        "class FinalEnsemble:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'rf1': RandomForestClassifier(\n",
        "                n_estimators=200, max_depth=12, min_samples_leaf=6,\n",
        "                max_features=0.8, random_state=42, n_jobs=-1, class_weight='balanced'\n",
        "            ),\n",
        "            'et1': ExtraTreesClassifier(\n",
        "                n_estimators=200, max_depth=14, min_samples_leaf=4,\n",
        "                max_features=0.9, random_state=42, n_jobs=-1, class_weight='balanced'\n",
        "            ),\n",
        "            'rf2': RandomForestClassifier(\n",
        "                n_estimators=150, max_depth=8, min_samples_leaf=12,\n",
        "                max_features=0.6, random_state=123, n_jobs=-1, class_weight='balanced'\n",
        "            )\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y, term_id):\n",
        "        print(f\"\\nğŸ¯ Term {term_id} - æœ€çµ‚èª¿æ•´å­¦ç¿’\")\n",
        "        print(f\"ãƒ‡ãƒ¼ã‚¿: {X.shape}, é™½æ€§ç‡: {y.mean():.4f}\")\n",
        "\n",
        "        cv_scores = {}\n",
        "        for name, model in self.models.items():\n",
        "            model.fit(X, y)\n",
        "            cv_score = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n",
        "            cv_scores[name] = cv_score.mean()\n",
        "            print(f\"  {name}: {cv_score.mean():.4f} (Â±{cv_score.std():.3f})\")\n",
        "\n",
        "        scores = np.array(list(cv_scores.values()))\n",
        "        exp_scores = np.exp(scores * 3)\n",
        "        self.weights = dict(zip(cv_scores.keys(), exp_scores / exp_scores.sum()))\n",
        "        print(f\"  æœ€çµ‚é‡ã¿: {self.weights}\")\n",
        "\n",
        "        return cv_scores\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        predictions = {}\n",
        "        for name, model in self.models.items():\n",
        "            predictions[name] = model.predict_proba(X)[:, 1]\n",
        "\n",
        "        ensemble_pred = sum(\n",
        "            self.weights[name] * predictions[name]\n",
        "            for name in self.models.keys()\n",
        "        )\n",
        "        return ensemble_pred\n",
        "\n",
        "# ğŸš€ æœ€çµ‚ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
        "def final_pipeline(train_df, test_df):\n",
        "    \"\"\"æœ€çµ‚èª¿æ•´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\"\"\"\n",
        "    print(\"ğŸš€ æœ€çµ‚èª¿æ•´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ0.65é”æˆç‹™ã„ï¼‰\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # æœ€çµ‚ç‰¹å¾´é‡ä½œæˆ\n",
        "    train_final = create_final_features(train_df.copy())\n",
        "    test_final = create_final_features(test_df.copy())\n",
        "\n",
        "    # æœ€å¼·ç‰¹å¾´é‡ã‚»ãƒƒãƒˆ\n",
        "    final_features = [\n",
        "        # åŸºæœ¬é‡è¦ç‰¹å¾´é‡\n",
        "        'æ™‚ä¾¡ä¾¡é¡', 'é¡§å®¢ãƒ©ãƒ³ã‚¯', 'å¹´é½¢Ã—è³‡ç”£è¦æ¨¡', 'éå»å–å¼•å›æ•°',\n",
        "        'è³‡ç”£è¦æ¨¡', 'æŠ•è³‡åŠ¹ç‡', 'å¹´é½¢èª¿æ•´è³‡ç”£', 'å«ã¿æç›Šç‡',\n",
        "\n",
        "        # å®Ÿãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡\n",
        "        'å•†å“å¤šæ§˜æ€§', 'ãƒ‡ã‚¸ã‚¿ãƒ«åˆ©ç”¨ç‡', 'æŠ•è³‡æˆåŠŸä½“é¨“', 'éå»ç´¯ç©æç›Š',\n",
        "        'ãƒªã‚¹ã‚¯ç®¡ç†åº¦', 'ãƒªã‚¹ã‚¯æŒ‡å‘åº¦', 'å††å»ºã¦æ¯”ç‡',\n",
        "\n",
        "        # ã€æ–°è¦ã€‘æœ€çµ‚èª¿æ•´ç‰¹å¾´é‡\n",
        "        'æŠ•è³‡åŠ¹ç‡Ã—å¹´é½¢', 'ãƒªã‚¹ã‚¯Ã—çµŒé¨“', 'è³‡ç”£Ã—å–å¼•é »åº¦',\n",
        "        'æˆåŠŸç‡', 'åŠ¹ç‡ãƒ©ãƒ³ã‚¯', 'å¹´é½¢å†…è³‡ç”£é †ä½', 'ç·åˆæŠ•è³‡åŠ›',\n",
        "        'ç•°å¸¸é«˜è³‡ç”£', 'è¶…ã‚¢ã‚¯ãƒ†ã‚£ãƒ–',\n",
        "\n",
        "        # åŸºæœ¬æƒ…å ±\n",
        "        'é¡§å®¢å¹´é½¢', 'æŠ•è³‡çµŒé¨“ï¼ˆæ ªå¼ï¼‰', 'year', 'æŠ•è³‡æ–¹é‡',\n",
        "\n",
        "        # ãƒ•ãƒ©ã‚°\n",
        "        'é«˜è³‡ç”£ãƒ•ãƒ©ã‚°', 'å«ã¿æãƒ•ãƒ©ã‚°', 'ã‚·ãƒ‹ã‚¢ãƒ•ãƒ©ã‚°', 'æ ªå¼çµŒé¨“ã‚ã‚Š'\n",
        "    ]\n",
        "\n",
        "    print(f\"ğŸ“Š æœ€çµ‚ç‰¹å¾´é‡: {len(final_features)}å€‹\")\n",
        "\n",
        "    # Termåˆ¥äºˆæ¸¬\n",
        "    all_predictions = pd.DataFrame()\n",
        "    term_aucs = []\n",
        "\n",
        "    for term_id in range(1, 7):\n",
        "        print(f\"\\n{'='*40}\")\n",
        "        print(f\"Term {term_id} æœ€çµ‚èª¿æ•´\")\n",
        "        print(f\"{'='*40}\")\n",
        "\n",
        "        # ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
        "        train_term = train_final[train_final[f'train_term_{term_id}'] == 1]\n",
        "        test_term = test_final[test_final[f'test_term_{term_id}'] == 1]\n",
        "\n",
        "        available_features = [f for f in final_features if f in train_term.columns]\n",
        "        print(f\"åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: {len(available_features)}å€‹\")\n",
        "\n",
        "        X_train = train_term[available_features].fillna(0)\n",
        "        y_train = train_term['y']\n",
        "        X_test = test_term[available_features].fillna(0)\n",
        "\n",
        "        # ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
        "        categorical_cols = ['æŠ•è³‡æ–¹é‡', 'æŠ•è³‡å®¶ã‚¿ã‚¤ãƒ—', 'ä¸»è¦ãƒªã‚¹ã‚¯ã‚«ãƒ†ã‚´ãƒª']\n",
        "        categorical_to_encode = [col for col in categorical_cols if col in X_train.columns]\n",
        "\n",
        "        if categorical_to_encode:\n",
        "            X_train = pd.get_dummies(X_train, columns=categorical_to_encode, drop_first=True, dtype=int)\n",
        "            X_test = pd.get_dummies(X_test, columns=categorical_to_encode, drop_first=True, dtype=int)\n",
        "\n",
        "            missing_cols = set(X_train.columns) - set(X_test.columns)\n",
        "            for col in missing_cols:\n",
        "                X_test[col] = 0\n",
        "            X_test = X_test[X_train.columns]\n",
        "\n",
        "        print(f\"æœ€çµ‚ç‰¹å¾´é‡æ•°: {X_train.shape[1]}å€‹\")\n",
        "\n",
        "        # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\n",
        "        ensemble = FinalEnsemble()\n",
        "        cv_scores = ensemble.fit(X_train, y_train, term_id)\n",
        "        term_aucs.append(np.mean(list(cv_scores.values())))\n",
        "\n",
        "        # äºˆæ¸¬\n",
        "        test_pred = ensemble.predict_proba(X_test)\n",
        "\n",
        "        # çµæœä¿å­˜\n",
        "        pred_df = pd.DataFrame({'ID': test_term['ID'], 'predict': test_pred})\n",
        "        all_predictions = pd.concat([all_predictions, pred_df]) if not all_predictions.empty else pred_df\n",
        "\n",
        "    # æœ€çµ‚çµæœ\n",
        "    avg_auc = np.mean(term_aucs)\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(\"ğŸ¯ æœ€çµ‚èª¿æ•´çµæœ\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for i, auc in enumerate(term_aucs, 1):\n",
        "        print(f\"Term {i}: {auc:.4f}\")\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ æœ€çµ‚CV-AUC: {avg_auc:.4f}\")\n",
        "    print(f\"ğŸ“Š äºˆæƒ³ãƒ‘ãƒ–ãƒªãƒƒã‚¯: {avg_auc - 0.027:.4f}\")\n",
        "\n",
        "    if avg_auc >= 0.677:\n",
        "        print(\"ğŸ‰ 0.65é”æˆå¯èƒ½æ€§å¤§ï¼\")\n",
        "    elif avg_auc >= 0.67:\n",
        "        print(\"âœ… 0.64å¾ŒåŠé”æˆï¼\")\n",
        "\n",
        "    return all_predictions, avg_auc"
      ],
      "metadata": {
        "id": "NcCukdSs8e22"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®å¼·åŒ–"
      ],
      "metadata": {
        "id": "iRopblc5K2pO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–¢æ•°\n",
        "def evaluate_model_performance(y_true, y_pred_proba, threshold=0.23):\n",
        "    \"\"\"PoCã§è¨­å®šã—ãŸæŒ‡æ¨™ã§è©•ä¾¡\"\"\"\n",
        "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
        "\n",
        "    metrics = {\n",
        "        'AUC': roc_auc_score(y_true, y_pred_proba),\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'F1': f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "    print(f\"AUC: {metrics['AUC']:.4f} (ç›®æ¨™: 0.7ä»¥ä¸Š)\")\n",
        "    print(f\"Precision: {metrics['Precision']:.4f} (ç›®æ¨™: 0.4ä»¥ä¸Š)\")\n",
        "    print(f\"Recall: {metrics['Recall']:.4f} (ç›®æ¨™: 0.6ä»¥ä¸Š)\")\n",
        "    print(f\"F1 Score: {metrics['F1']:.4f} (ç›®æ¨™: 0.5ä»¥ä¸Š)\")\n",
        "\n",
        "    return metrics"
      ],
      "metadata": {
        "id": "sgDwaXdXK1pl"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ”¹å–„ç‰ˆï¼‰"
      ],
      "metadata": {
        "id": "-UEuTjAjK-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸš€ æœ€çµ‚èª¿æ•´å®Ÿè¡Œï¼ˆ0.65é”æˆç‹™ã„ï¼‰\")\n",
        "final_predictions, final_auc = final_pipeline(train_df, test_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vfl0gesBLDg_",
        "outputId": "6cff5a87-85d2-40c5-862d-d65b66e2f05c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ æœ€çµ‚èª¿æ•´å®Ÿè¡Œï¼ˆ0.65é”æˆç‹™ã„ï¼‰\n",
            "ğŸš€ æœ€çµ‚èª¿æ•´ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆ0.65é”æˆç‹™ã„ï¼‰\n",
            "============================================================\n",
            "ğŸ“Š æœ€çµ‚ç‰¹å¾´é‡: 32å€‹\n",
            "\n",
            "========================================\n",
            "Term 1 æœ€çµ‚èª¿æ•´\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 32å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 33å€‹\n",
            "\n",
            "ğŸ¯ Term 1 - æœ€çµ‚èª¿æ•´å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (80000, 33), é™½æ€§ç‡: 0.1972\n",
            "  rf1: 0.6689 (Â±0.019)\n",
            "  et1: 0.6676 (Â±0.016)\n",
            "  rf2: 0.6849 (Â±0.014)\n",
            "  æœ€çµ‚é‡ã¿: {'rf1': np.float64(0.3283445751389319), 'et1': np.float64(0.3271140039404394), 'rf2': np.float64(0.3445414209206287)}\n",
            "\n",
            "========================================\n",
            "Term 2 æœ€çµ‚èª¿æ•´\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 32å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 33å€‹\n",
            "\n",
            "ğŸ¯ Term 2 - æœ€çµ‚èª¿æ•´å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (82000, 33), é™½æ€§ç‡: 0.1976\n",
            "  rf1: 0.6680 (Â±0.019)\n",
            "  et1: 0.6667 (Â±0.015)\n",
            "  rf2: 0.6831 (Â±0.016)\n",
            "  æœ€çµ‚é‡ã¿: {'rf1': np.float64(0.3286809601766906), 'et1': np.float64(0.32739025581951753), 'rf2': np.float64(0.34392878400379195)}\n",
            "\n",
            "========================================\n",
            "Term 3 æœ€çµ‚èª¿æ•´\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 32å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 33å€‹\n",
            "\n",
            "ğŸ¯ Term 3 - æœ€çµ‚èª¿æ•´å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (84000, 33), é™½æ€§ç‡: 0.1980\n",
            "  rf1: 0.6613 (Â±0.019)\n",
            "  et1: 0.6668 (Â±0.016)\n",
            "  rf2: 0.6761 (Â±0.018)\n",
            "  æœ€çµ‚é‡ã¿: {'rf1': np.float64(0.32655385997904557), 'et1': np.float64(0.3320207911316294), 'rf2': np.float64(0.34142534888932496)}\n",
            "\n",
            "========================================\n",
            "Term 4 æœ€çµ‚èª¿æ•´\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 32å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 33å€‹\n",
            "\n",
            "ğŸ¯ Term 4 - æœ€çµ‚èª¿æ•´å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (86000, 33), é™½æ€§ç‡: 0.1985\n",
            "  rf1: 0.6670 (Â±0.016)\n",
            "  et1: 0.6652 (Â±0.012)\n",
            "  rf2: 0.6769 (Â±0.018)\n",
            "  æœ€çµ‚é‡ã¿: {'rf1': np.float64(0.3306157065780237), 'et1': np.float64(0.3288115264962616), 'rf2': np.float64(0.3405727669257148)}\n",
            "\n",
            "========================================\n",
            "Term 5 æœ€çµ‚èª¿æ•´\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 32å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 33å€‹\n",
            "\n",
            "ğŸ¯ Term 5 - æœ€çµ‚èª¿æ•´å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (88000, 33), é™½æ€§ç‡: 0.1989\n",
            "  rf1: 0.6635 (Â±0.016)\n",
            "  et1: 0.6612 (Â±0.013)\n",
            "  rf2: 0.6733 (Â±0.018)\n",
            "  æœ€çµ‚é‡ã¿: {'rf1': np.float64(0.3307511573072268), 'et1': np.float64(0.3285546370787071), 'rf2': np.float64(0.34069420561406594)}\n",
            "\n",
            "========================================\n",
            "Term 6 æœ€çµ‚èª¿æ•´\n",
            "========================================\n",
            "åˆ©ç”¨å¯èƒ½ç‰¹å¾´é‡: 32å€‹\n",
            "æœ€çµ‚ç‰¹å¾´é‡æ•°: 33å€‹\n",
            "\n",
            "ğŸ¯ Term 6 - æœ€çµ‚èª¿æ•´å­¦ç¿’\n",
            "ãƒ‡ãƒ¼ã‚¿: (90000, 33), é™½æ€§ç‡: 0.1989\n",
            "  rf1: 0.6625 (Â±0.016)\n",
            "  et1: 0.6639 (Â±0.016)\n",
            "  rf2: 0.6725 (Â±0.017)\n",
            "  æœ€çµ‚é‡ã¿: {'rf1': np.float64(0.3294972311874377), 'et1': np.float64(0.3309116008039361), 'rf2': np.float64(0.3395911680086262)}\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ æœ€çµ‚èª¿æ•´çµæœ\n",
            "============================================================\n",
            "Term 1: 0.6738\n",
            "Term 2: 0.6726\n",
            "Term 3: 0.6681\n",
            "Term 4: 0.6697\n",
            "Term 5: 0.6660\n",
            "Term 6: 0.6663\n",
            "\n",
            "ğŸ“ˆ æœ€çµ‚CV-AUC: 0.6694\n",
            "ğŸ“Š äºˆæƒ³ãƒ‘ãƒ–ãƒªãƒƒã‚¯: 0.6424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ\n",
        "submission_df[1] = final_predictions['predict']\n",
        "submission_df.to_csv(base_dir + '/final_submission.csv', index=False, header=False)\n",
        "\n",
        "print(f\"\\nğŸ¯ æœ€çµ‚èª¿æ•´å®Œäº†ï¼\")\n",
        "print(f\"CV-AUC: {final_auc:.4f}\")\n",
        "print(f\"äºˆæƒ³æ”¹å–„: 0.6381 â†’ {final_auc - 0.027:.4f}\")\n",
        "print(\"æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: final_submission.csv\")"
      ],
      "metadata": {
        "id": "GMqCAoZVPyUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468a6d5e-6b25-4843-b8cb-9a7eb1df831a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ æœ€çµ‚èª¿æ•´å®Œäº†ï¼\n",
            "CV-AUC: 0.6694\n",
            "äºˆæƒ³æ”¹å–„: 0.6381 â†’ 0.6424\n",
            "æå‡ºãƒ•ã‚¡ã‚¤ãƒ«: final_submission.csv\n"
          ]
        }
      ]
    }
  ]
}